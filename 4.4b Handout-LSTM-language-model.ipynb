{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM language model handout\n",
    "## [COSC 7336 Advanced Natural Language Processing](https://fagonzalezo.github.io/dl-tau-2017-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this handout we will build a character-based language model using a Long Short Term Memory (LSTM) recurrent neural network. The code is based on this Keras example https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py.\n",
    "\n",
    "We will use a text from Nietzsche that is availabe here: https://s3.amazonaws.com/text-datasets/nietzsche.txt. The text is converted to lowercase and we build a two dictionaries to map characters to indices and back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chars: 600893\n",
      "Vocabulary size: 57\n"
     ]
    }
   ],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"Total number of chars:\", len(text))\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of the test in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts object purely and simply as \"the thing in itself,\" without any\n",
      "falsification taking place either on the part of the subject or the\n",
      "object. i would repeat it, however, a hundred times, that \"immediate\n",
      "certainty,\" as well as \"absolute knowledge\" and the \"thing in itself,\"\n",
      "involve a contradictio in adjecto; we really ought to free ourselves\n",
      "from the misleading significance of words! the people on their part may\n",
      "think that cognition is knowing all about things, but the philosopher\n",
      "must say to him\n"
     ]
    }
   ],
   "source": [
    "print(text[31000:31500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Many to one LSTM model \n",
    "\n",
    "We will build a model with this structure:\n",
    "\n",
    "![many-to-one.jpg](many-to-one.jpg)\n",
    "\n",
    "The model receives sequences of size 40 and predicts the character that will follow this sequence, the 41-th character. The LSTM layer will have 128 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n",
      "Layer (type)                   Output Shape                Param #    \n",
      "======================================================================\n",
      "lstm_1 (LSTM)                  (None, 128)                 95232      \n",
      "______________________________________________________________________\n",
      "dense_1 (Dense)                (None, 57)                  7353       \n",
      "______________________________________________________________________\n",
      "activation_1 (Activation)      (None, 57)                  0          \n",
      "======================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, vocab_size), return_sequences=False, name=\"lstm_1\"))\n",
    "model.add(Dense(vocab_size, name=\"dense_1\"))\n",
    "model.add(Activation('softmax', name=\"activation_1\"))\n",
    "model.summary(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train this model we need to build the sequences from the input text. Each sequence will have a length of `maxlen = 40` and the label will correspond to the character that follows that sequence, i.e. the 41-th character. The sequences overlap, this is controlled with the `step` variable. Characters are represented using a one-hot representation of `len(chars)` length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 200285\n",
      "Vectorization...\n",
      "Shape X (200285, 40, 57)\n",
      "Shape y (200285, 57)\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('Shape X', X.shape)\n",
    "print('Shape y', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a categorical crossentropy loss since the output of the model is a softmax layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model will require at least 20 epochs to get good results, so a GPU is a most. For illustration we will train it for just an epoch with a reduced set of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s - loss: 3.4797     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125140be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[:1000,:,:], y[:1000,:], batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Many-to-many LSTM\n",
    "\n",
    "Despite the model we trained is a many-to-one model, we can convert it to a many-to-many model, since an LSTM always produces an output for every time step:\n",
    "\n",
    "![many-to-many.jpg](many-to-many.jpg)\n",
    "\n",
    "In order to do this we will change the architecture of the model. In particular, we will change the dense output layer so that it produces an output for every time step. This is accomplished by using the `TimeDistributed` class. This doesn't increase the number of parameters since the weights are shared for all the time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n",
      "Layer (type)                   Output Shape                Param #    \n",
      "======================================================================\n",
      "lstm_1 (LSTM)                  (None, 40, 128)             95232      \n",
      "______________________________________________________________________\n",
      "dense_1 (TimeDistributed)      (None, 40, 57)              7353       \n",
      "______________________________________________________________________\n",
      "activation_1 (Activation)      (None, 40, 57)              0          \n",
      "======================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, vocab_size), return_sequences=True, name=\"lstm_1\"))\n",
    "model.add(TimeDistributed(Dense(vocab_size), name=\"dense_1\"))#Check names to see how to load weights\n",
    "model.add(Activation('softmax', name=\"activation_1\"))\n",
    "model.summary(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load a pretrained model. This model corresponds to the many-to-one model define before, but since the parameters of both models are the same we can use the weights for the many-to-many model. You can obtain the model here: https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/lstm-pretrained-weights.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file = 'lstm-pretrained-weights.hdf5'\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.load_weights(h5file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating the probability of a text\n",
    "\n",
    "The model that we trained calculates the probability of a character given the previous characters: $P(c_i | c_{1},\\dots, c_{i-1})$. We can use this conditional probability to calculate the join probability of a given sequence as follows:\n",
    "$$P(c_1, \\dots, c_n) = P(c_1)\\prod_{i=2}^{n}\\ P(c_i | c_{1},\\dots, c_{i-1})$$\n",
    "\n",
    "This probability is the likelihood of the particular sequence $c_1, \\dots, c_n$ being generated by the language model.\n",
    "\n",
    "In the many-to-many model the conditional probabilities are given by the outputs at each time step. Remember that the output of the model is a softmax layer with as many neurons are characters in the vocabulary. The following function calculates the log likelihood of a text according to the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(model, text):\n",
    "    probs = model.predict(parse_text(text, vocab_size, padding=True)).squeeze()\n",
    "    return sum([np.log(probs[i, char_indices[c]]) \n",
    "                 for i,c in enumerate(text[1:]) ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it for particular texts, we need to represent them in a way that is understood by the neural network model, this is done by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(text, vocab_size, padding=False):\n",
    "    if padding:\n",
    "        X = np.zeros((1, maxlen, vocab_size), dtype=np.bool)\n",
    "    else:\n",
    "        X = np.zeros((1, len(text), vocab_size), dtype=np.bool)\n",
    "    for t, char in enumerate(text):\n",
    "        X[0, t, char_indices[char]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute value of the likelihood does not say much by itself. It is more useful if we use it to compare different texts, i.e. if we interpret it in relative terms.\n",
    "\n",
    "For instance the likelihood of the text *\"the thing in itself\"* is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.6521512464\n"
     ]
    }
   ],
   "source": [
    "print (log_likelihood(model, \"the thing in itself\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the likelihood of the same text without spaces, *\"thethinginitself\"*, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-59.0525213182\n"
     ]
    }
   ],
   "source": [
    "print (log_likelihood(model, \"thethinginitself\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that $$P(\\text{\"the thing in itself\"} | model) > P(\\text{\"thethinginitself\"} | model)$$\n",
    "\n",
    "The following are other variations of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-30.8013833459\n",
      "-31.7916916162\n"
     ]
    }
   ],
   "source": [
    "print (log_likelihood(model, \"the thingy in itself\"))\n",
    "print (log_likelihood(model, \"itself thing the in\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggest that the model is able to capture different aspects of the text structure such as the morphology of the words or the syntaxis.\n",
    "\n",
    "### 3.1 Morphological structure\n",
    "\n",
    "The model can help us to identify the most likely (unlikely) combinations of a set of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.90723706037 y wh\n",
      "-4.96534752846 why \n",
      "-7.63534402847  why\n",
      "-7.64063081145 hy w\n",
      "-9.28439575434 y hw\n",
      "--------------------------------------------------\n",
      "-18.8574256897  ywh\n",
      "-19.5259504318 wyh \n",
      "-21.4377524853 h wy\n",
      "-21.4675536156 h yw\n",
      "-25.4104990959  yhw\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "char_list = list(u'ywh ')\n",
    "perms = [''.join(perm) for perm in permutations(char_list)]\n",
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Syntactical structure\n",
    "\n",
    "Instead of characters we can use words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "bow =  ['philosopher', 'kant', 'is', 'a']\n",
    "perms = [' '+' '.join(perm)+' ' for perm in permutations(bow)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most likely permutations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.924826585  is a philosopher kant \n",
      "-32.3237102695  kant is a philosopher \n",
      "-34.8795643604  a kant is philosopher \n",
      "-35.6037164213  kant is philosopher a \n",
      "-39.9717190545  a philosopher kant is \n",
      "-43.2843809983  is philosopher kant a \n",
      "-43.6753551863  is kant a philosopher \n",
      "-43.7594629446  a kant philosopher is \n",
      "-43.8379937951  is a kant philosopher \n",
      "-44.6056019792  a is kant philosopher \n"
     ]
    }
   ],
   "source": [
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse = True)[:10]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most unlikely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-50.7616942155  kant a is philosopher \n",
      "-51.1649401682  kant philosopher is a \n",
      "-53.6018530477  is kant philosopher a \n",
      "-58.6275342517  philosopher kant is a \n",
      "-65.4431888871  philosopher is a kant \n",
      "-66.7852299176  philosopher a kant is \n",
      "-69.441136879  kant philosopher a is \n",
      "-71.2249834351  philosopher is kant a \n",
      "-71.6653220765  philosopher kant a is \n",
      "-76.0457286723  philosopher a is kant \n"
     ]
    }
   ],
   "source": [
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse = True)[-10:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating text\n",
    "\n",
    "One of the most interesting applications of language models is random text generation. \n",
    "\n",
    "Since the model tells us the probability of the next character given the previous ones. We can start from a given text and use the model conditional probability to get a good candidate for the next character, we add that character to our sequence and we keep generating characters in this way.\n",
    "\n",
    "To generate a character using the conditional probability calculated by the model, $P(c_t | c_{1}, c_{2},\\dots, c_{t-1})$, we will use two main mechanisms: deterministic and  stochastic. For the deterministic we generate the character with the maximum probability. For the stochastic we sample from the conditional probability distribution given by the model. We combine the two strategies using a process analogous to this one:\n",
    "  ```python\n",
    "  for i in [1..n]:\n",
    "      P = predict_next() \n",
    "      bin_var = sample_binomial(temperature)\n",
    "      if bin_var:\n",
    "          c_i = sample_multinomial(P) \n",
    "      else:\n",
    "          c_i = P.argmax() \n",
    "  ```\n",
    "The `temperature` parameter determines the balance between stochastic and deterministic. A higher value makes the process more stochastic, a lower value makes it more deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to sample an index from a probability array:\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function feeds the model with an initial text, successively execute the model to get a probability distribution and uses the `sample` function to generate the next character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(diversity, model, sentence, n_chars, padding=True):\n",
    "    print()\n",
    "    print(sentence, end='')\n",
    "    for i in range(n_chars):\n",
    "        x = np.zeros((1, maxlen, vocab_size))\n",
    "        if padding and len(sentence) < 40:\n",
    "            space_array = [\" \"]*(40-len(sentence))\n",
    "            for t, char in enumerate(space_array):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "        for t, char in enumerate(sentence, 40-len(sentence)):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds[-1], diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask the model for the *'the meaning of life'* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the meaning of life is the the idea of the whole pain\n",
      "empterial into the same ponithing\" as they she\n",
      "tainst the such sense\n",
      "of the whole misherer and dreams quallity but new learts the most sufficiently\n",
      "remends the his not a religion belief in the streels have the experience man condixipe\" \"and lackernness,\n",
      "sit of the hild \"signintilly in hordinessy of\n",
      "disclurs one\" there is among morals \"to present in the proporishing; "
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(0.5, model, 'the meaning of life is ', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
