{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data selection\n",
    "\n",
    "Juan Navarro, <jsnavarroa@unal.edu.co>\n",
    "\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the data\n",
    "\n",
    "Use Selenium to download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "chrome_options = webdriver.chrome.options.Options()  \n",
    "#chrome_options.add_argument(\"--headless\")\n",
    "download_path = os.path.abspath(\"./data/\")\n",
    "prefs = {\"download.default_directory\" : download_path}\n",
    "chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "browser.implicitly_wait(30)\n",
    "\n",
    "def is_element_present(how, what):\n",
    "    try: \n",
    "        browser.find_element(by=how, value=what)\n",
    "    except NoSuchElementException as e: \n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def waitForElementPresent(how, what):\n",
    "    for i in range(60):\n",
    "        if is_element_present(how, what): \n",
    "                break\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Get the page\n",
    "browser.get(\"https://www.superfinanciera.gov.co/ReporteInformacionFIC/faces/reporte/consultaReporte.xhtml\")\n",
    "# Select the search by name\n",
    "browser.find_element_by_xpath(\"//table[@id='formaDatosReporte:tipoBusqueda']/tbody/tr/td[3]/div/div[2]/span\").click()\n",
    "# Wait for name field\n",
    "waitForElementPresent(By.ID, \"formaDatosReporte:nombreEntidadInput_input\")\n",
    "# Write name\n",
    "browser.find_element_by_id(\"formaDatosReporte:nombreEntidadInput_input\").send_keys(\"FIDUCIARIA BANCOLOMBIA S.A.\")\n",
    "# Accept the first suggestion\n",
    "browser.find_element_by_xpath(\"//div[@id='formaDatosReporte:nombreEntidadInput_panel']/ul/li/span\").click()\n",
    "# Write initial date\n",
    "browser.find_element_by_id(\"formaDatosReporte:fechaReporteInicial_input\").send_keys(\"01/05/2018\")\n",
    "# Write final date\n",
    "browser.find_element_by_id(\"formaDatosReporte:fechaReporteFinal_input\").send_keys(\"28/05/2018\")\n",
    "# Begin the search\n",
    "browser.find_element_by_xpath(\"//button[@id='formaDatosReporte:cmdReporte']/span\").click()\n",
    "\n",
    "while(True):\n",
    "    # Download report\n",
    "    browser.find_element_by_xpath(\"//button[@id='formaResultadoRentabilidad:j_id_1u']/span[2]\").click()\n",
    "    # Go to next report page\n",
    "    paginator = browser.find_element_by_xpath(\"//div[@id='formaResultadoRentabilidad:tblResultadoReporte_paginator_bottom']/span[4]\")\n",
    "    paginator.click()\n",
    "    if \"ui-state-disabled\" in paginator.get_attribute(\"class\"):\n",
    "        break\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combine all the data in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "vSigmoid = np.vectorize(sigmoid)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def forward_propagate(x, w):\n",
    "    '''\n",
    "    x: input value for neuron 5\n",
    "    w: weights array in the following order\n",
    "       [w13, w12, w14, w32, w42, w53, w54]\n",
    "    Returns: a pair of arrays (a, z), where \n",
    "             a has the activation values for each neuron, and\n",
    "             z the output values for each neuron\n",
    "    '''\n",
    "    z = np.zeros(5)\n",
    "    a = np.zeros(5)\n",
    "    # your code here\n",
    "    \n",
    "    w13, w12, w14, w32, w42, w53, w54 = w\n",
    "    a1, a2, a3, a4, a5 = a\n",
    "    z1, z2, z3, z4, z5 = z\n",
    "    \n",
    "    a5 = x\n",
    "    z5 = a5\n",
    "    \n",
    "    a[2:4] = np.dot(z5, w[5:7])\n",
    "    z[2:4] = vSigmoid(a[2:4])\n",
    "    a3, a4 = a[2:4]\n",
    "    z3, z4 = z[2:4]\n",
    "    \n",
    "    \n",
    "    a2 = w32*z3 + w42*z4\n",
    "    z2 = relu(a2)\n",
    "    \n",
    "    a1 = w12*z2 + w13*z3 + w14*z4\n",
    "    z1 = sigmoid(a1) \n",
    "    \n",
    "    a = np.array([a1, a2, a3, a4, a5])\n",
    "    z = np.array([z1, z2, z3, z4, z5])\n",
    "    \n",
    "    return (a, z)\n",
    "\n",
    "x = 1\n",
    "w = [1, 2, 1, 0, 3, 1, 2]\n",
    "a, z = forward_propagate(x, w)\n",
    "\n",
    "test_a = np.array([6.89663812, 2.64239123, 1.        , 2.        , 1.        ])\n",
    "test_z = np.array([0.99898984, 2.64239123, 0.73105858, 0.88079708, 1.        ])\n",
    "    \n",
    "print(\"Test a:\", np.isclose(test_a, a))\n",
    "print(\"Test z:\", np.isclose(test_z, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b Deduce the equations to calculate $\\delta_{i}$ (the error value per\n",
    "neuron) for all the neurons. Write a function that given a training\n",
    "sample and the weights of the network calculate $\\delta_{i}$ for\n",
    "each neuron. Assume a square error loss:\n",
    "\n",
    "$$ L_2(f, D) =\\sum_{(x_{i},r_{i})\\in D} (r_i - f(x_i))^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_error(y_true, y_pred):\n",
    "    return (y_true - y_pred)**2\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def relu_derivative(x):\n",
    "    if(x > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def bp(x, y, w):\n",
    "    '''\n",
    "    x: input value for neuron 5\n",
    "    y: output value for neuron 1\n",
    "    w: weights array in the following order\n",
    "       [w13, w12, w14, w32, w42, w53, w54]\n",
    "    Returns: an array delta with the delta values for each\n",
    "             neuron\n",
    "    '''\n",
    "    delta = np.zeros(5)\n",
    "    ##\n",
    "    a, z = forward_propagate(x, w)\n",
    "    \n",
    "    a1, a2, a3, a4, a5 = a\n",
    "    z1, z2, z3, z4, z5 = z\n",
    "    w13, w12, w14, w32, w42, w53, w54 = w\n",
    "    d1, d2, d3, d4, d5 = delta\n",
    "        \n",
    "    d1 = sigmoid_derivative(a1) * (z1 - y)\n",
    "    \n",
    "    d2 = relu_derivative(a2) * (d1*w12)\n",
    "    \n",
    "    d3 = sigmoid_derivative(a3) * (d1*w13 + d2*w32)\n",
    "    \n",
    "    d4 = sigmoid_derivative(a4) * (d1*w14 + d2*w42)\n",
    "    \n",
    "    d5 = (d3*w53 + d4*w54)\n",
    "    \n",
    "    delta = np.array([d1, d2, d3, d4, d5])\n",
    "    \n",
    "    ##\n",
    "    return delta\n",
    "\n",
    "\n",
    "x=-3.0 \n",
    "y=0.731204118803\n",
    "w = [1, 2, 1, 0, 3, 1, 2]\n",
    "bp(x, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c Write a function to update the neural network weights when a new training sample is shown using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, w, eta):\n",
    "    '''\n",
    "    x: input value for neuron 5\n",
    "    y: output value for neuron 1\n",
    "    w: weights array in the following order\n",
    "       [w13, w12, w14, w32, w42, w53, w54]\n",
    "    eta: learning rate\n",
    "    Returns: updated w array\n",
    "    '''\n",
    "    # Calculate dw\n",
    "    dw = np.zeros(7)\n",
    "    \n",
    "    # Your code here\n",
    "    a, z = forward_propagate(x, w)\n",
    "    \n",
    "    delta = bp(x, y, w)\n",
    "    \n",
    "    z1, z2, z3, z4, z5 = z\n",
    "    dw13, dw12, dw14, dw32, dw42, dw53, dw54 = dw\n",
    "    d1, d2, d3, d4, d5 = delta\n",
    "    \n",
    "    dw13 = d1 * z3\n",
    "    dw12 = d1 * z2\n",
    "    dw14 = d1 * z4\n",
    "    dw32 = d3 * z2\n",
    "    dw42 = d4 * z2\n",
    "    dw53 = d5 * z3\n",
    "    dw54 = d5 * z4\n",
    "    \n",
    "    dw = np.array([dw13, dw12, dw14, dw32, dw42, dw53, dw54])\n",
    "    \n",
    "    ####  ####\n",
    "    w = w - eta*dw\n",
    "    return w\n",
    "\n",
    "x=-3.0 \n",
    "y=0.731204118803\n",
    "w = [1, 2, 1, 0, 3, 1, 2]\n",
    "eta = 0.1\n",
    "train_step(x, y, w, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.d Use the previous function to train the network with these [training samples](http://fagonzalezo.github.io/ml/samples_assign4.txt).  Plot the evolution of the error and the predictions of the trained network. Write down the weights of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Get the data set\n",
    "path = get_file(os.path.abspath(\"./samples_assign4.txt\"), \n",
    "                origin=\"http://fagonzalezo.github.io/ml/samples_assign4.txt\" )\n",
    "\n",
    "data = pd.read_csv(path, sep=\" \", header=None)\n",
    "\n",
    "X = data.values[:, 0]\n",
    "Y = data.values[:, 1]\n",
    "\n",
    "# Classification function\n",
    "def predict(X, w):\n",
    "    Y_pred = []\n",
    "    for x in X:\n",
    "        a, z = forward_propagate(x, w)\n",
    "        Y_pred.append(z[0])\n",
    "    return Y_pred\n",
    "\n",
    "# Error function\n",
    "def evalOnDataSet(X, Y, w):\n",
    "    Y_pred = predict(X, w)\n",
    "    return mean_squared_error(Y, Y_pred)\n",
    "\n",
    "def experiment(X, Y, w, eta=0.01, epochs=2):\n",
    "    error = []\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in zip(X, Y):\n",
    "            w = train_step(x, y, w, eta)\n",
    "            error.append(evalOnDataSet(X, Y, w))\n",
    "    \n",
    "    Y_pred = predict(X, w)\n",
    "    return w, Y_pred, error\n",
    "\n",
    "# Training the network\n",
    "np.random.seed(42)\n",
    "initial_w = (np.random.rand(7)-0.5)*10\n",
    "etas = np.linspace(0, 1, num=5)\n",
    "etas = [0.1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8*2, 5))\n",
    "ax1.plot(X, Y, label='Original', color='black')\n",
    "\n",
    "for eta in etas:\n",
    "    w, Y_pred, error_by_iter = experiment(X, Y, initial_w, eta=eta, epochs=2*2)\n",
    "    print(\"(eta={})  w = {}\".format(eta, w))\n",
    "\n",
    "    # Plot the predictions\n",
    "    ax1.plot(X, Y_pred, label='Predicted (eta={})'.format(eta))\n",
    "    ax1.set_title(\"Predictions\")\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax1.legend()\n",
    "    \n",
    "    i = range(1, len(error_by_iter) +1)\n",
    "    ax2.set_title(\"Error vs Iterations\")\n",
    "    ax2.set_ylabel(\"Mean squeared error\")\n",
    "    ax2.plot(i, error_by_iter, label='eta={}'.format(eta))\n",
    "    ax2.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. (1.0) The hangman\n",
    "\n",
    "Design a function able to find the missing characters from a word. The function must work as follows:\n",
    "\n",
    "```\n",
    ">>> hangman(\"pe_p_e\")\n",
    "'people'\n",
    "\n",
    ">>> hangman(\"phi__sop_y\")\n",
    "'philosophy'\n",
    "\n",
    ">>> hangman(\"si_nif_c_nc_\")\n",
    "'significance'\n",
    "\n",
    ">>> hangman(\"kn__l_d_e\")\n",
    "'knowledge'\n",
    "\n",
    ">>> hangman(\"inte_r_ga_i_n\")\n",
    "'interrogation'\n",
    "```\n",
    "\n",
    "The function must be able to deal with up to 4 unknowns in arbitrary length words. The function must work in a reasonable time (max 30 seconds in a laptop). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Get the data\n",
    "path = get_file(os.path.abspath('./nietzsche.txt'), \n",
    "                origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"Total number of chars:\", len(text))\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Example: \", text[31000:31100])\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('Shape X', X.shape)\n",
    "print('Shape y', y.shape)\n",
    "\n",
    "# Build the model: Many-to-one LSTM\n",
    "h5file = 'lstm-weights.hdf5'\n",
    "if not os.path.isfile(h5file):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(maxlen, vocab_size), return_sequences=False, name=\"lstm_1\"))\n",
    "    model.add(Dense(vocab_size, name=\"dense_1\"))\n",
    "    model.add(Activation('softmax', name=\"activation_1\"))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizer)\n",
    "\n",
    "    num_samples = 200285\n",
    "    model.fit(X[:num_samples,:,:], y[:num_samples,:], batch_size=128, epochs=20)\n",
    "\n",
    "    model.save_weights(h5file)\n",
    "\n",
    "# Built the model: Many-to-many LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, vocab_size), return_sequences=True, name=\"lstm_1\"))\n",
    "model.add(TimeDistributed(Dense(vocab_size), name=\"dense_1\"))#Check names to see how to load weights\n",
    "model.add(Activation('softmax', name=\"activation_1\"))\n",
    "\n",
    "# Optimize the model\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.load_weights(h5file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, vocab_size, padding=False):\n",
    "    if padding:\n",
    "        X = np.zeros((1, maxlen, vocab_size), dtype=np.bool)\n",
    "    else:\n",
    "        X = np.zeros((1, len(text), vocab_size), dtype=np.bool)\n",
    "    for t, char in enumerate(text):\n",
    "        X[0, t, char_indices[char]] = 1\n",
    "    return X\n",
    "\n",
    "def log_likelihood(model, text):\n",
    "    probs = model.predict(parse_text(text, vocab_size, padding=True)).squeeze()\n",
    "    return sum([np.log(probs[i, char_indices[c]]) \n",
    "                 for i,c in enumerate(text[1:]) ])\n",
    "\n",
    "print (log_likelihood(model, \"the thing in itself\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', \n",
    "            'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "missing_char = '_'\n",
    "\n",
    "def hangman(word, beam_width=5, debug=False):\n",
    "    ### your code here\n",
    "    all_candidates = [[word, float('-inf')]]\n",
    "    \n",
    "    for i in range(word.count(missing_char)):\n",
    "        \n",
    "        next_candidates = []\n",
    "        missing_char_idx = all_candidates[0][0].find(missing_char)\n",
    "        \n",
    "        for candidate in all_candidates:\n",
    "            # replace the first missing char with an alphabet char\n",
    "            candidate_word = candidate[0]\n",
    "            for char in alphabet:\n",
    "                new_word = candidate_word.replace(missing_char, char, 1)\n",
    "                score = log_likelihood(model, new_word)\n",
    "                next_candidates.append([new_word, score])\n",
    "       \n",
    "        # order all candidates by score\n",
    "        ordered = sorted(next_candidates, key=itemgetter(1), reverse=True)\n",
    "        \n",
    "        # select beam_width best\n",
    "        all_candidates = ordered[:beam_width]\n",
    "        if(debug):\n",
    "            print(all_candidates)\n",
    "            \n",
    "    best = all_candidates[0]\n",
    "    \n",
    "    return best[0]\n",
    "\n",
    "hangman(\"_bj_ct\", beam_width=10, debug=True) # \"object\"\n",
    "#hangman(\"inte_r_ga_i_n\", beam_width=10, debug=True) # \"interrogation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def matching_error(a, b):\n",
    "    equal = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if x == y:\n",
    "            equal += 1\n",
    "    return 1-(equal/max(len(a), len(b)))\n",
    "\n",
    "vMatching_error = np.vectorize(matching_error)\n",
    "\n",
    "# Test values\n",
    "inputs =   [\"pe_p_e\", \"phi__sop_y\", \"si_nif_c_nc_\", \"kn__l_d_e\", \"inte_r_ga_i_n\"]\n",
    "expected = ['people', 'philosophy', 'significance', 'knowledge', 'interrogation']\n",
    "\n",
    "errors = []\n",
    "times = []\n",
    "\n",
    "# Call hangman with different k values\n",
    "K = np.arange(1, len(alphabet)+1, step=1)\n",
    "\n",
    "for beam_width in K:\n",
    "    outputs =[]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, c in zip(inputs, expected):\n",
    "        output = hangman(i, beam_width)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    exec_time = time.time() - start_time\n",
    "    error = np.mean(vMatching_error(expected, outputs))\n",
    "    times.append(exec_time)\n",
    "    errors.append(error)\n",
    "    print(\"beam_width = {},  error = {:.3f},  time(seconds) = {:.3f}\".format(beam_width, error, exec_time))\n",
    "    print(outputs)\n",
    "    print(expected)\n",
    "    \n",
    "    if(exec_time > 30):\n",
    "        break;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = K[:len(errors)]\n",
    "\n",
    "# Plot errors and times\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8*2, 5))\n",
    "\n",
    "ax1.set_title(\"Error vs beam_width\")\n",
    "ax1.plot(K, errors)\n",
    "ax1.scatter(K, errors)\n",
    "ax1.set_ylabel('Error')\n",
    "ax1.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "ax2.set_title(\"Time vs beam_width\")\n",
    "ax2.plot(K, times)\n",
    "ax2.scatter(K, times)\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (2.0) Bird classification. \n",
    "\n",
    "1. Direct prediction\n",
    "   * Download the dataset birds from http://www-cvr.ai.uiuc.edu/ponce_grp/data/.\n",
    "   * Use [Keras](https://keras.io/) and the [MobileNet](https://keras.io/applications/#mobilenet) pre-trained model, to classify the images in the birds dataset. Construct a confusion matrix that relates the bird classes with the 10 most frequent classes from ImageNet predicted by the model.\n",
    "2. Transfer learning\n",
    "   * Use the pre-trained MobileNet model as a feature extractor. Create a new model that replaces the top part of MobileNet with two layers of 256 and 6 neurons respectively.\n",
    "   * Change the attribute trainable of the other layers to be False. This will prevent the weights of these layers to be changed during training.\n",
    "   * Train the model with the training images from the bird dataset. \n",
    "   * Evaluate the performance over the test dataset reporting the results in a confusion matrix. Discuss the results. \n",
    "3. Fine tuning\n",
    "   * Repeat the experiment from the last question, but this time allow all the layers to be trained. \n",
    "   * Compare and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Get the data set\n",
    "birds_path = get_file(os.path.abspath('./birds.zip'), \n",
    "                origin=\"http://www-cvr.ai.uiuc.edu/ponce_grp/data/birds/birds.zip\")\n",
    "zip_ref = ZipFile(birds_path, 'r')\n",
    "zip_ref.extractall('./')\n",
    "zip_ref.close()\n",
    "\n",
    "# build data set\n",
    "paths = np.array([])\n",
    "y_true = np.array([])\n",
    "for root, dirs, files in os.walk('./birds/'):\n",
    "    for name in files:\n",
    "        label = os.path.basename(root)\n",
    "        path = os.path.join(root, name)\n",
    "        paths = np.append(paths, path)\n",
    "        y_true = np.append(y_true, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image #conda install pillow\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "# Load the model\n",
    "model = MobileNet()\n",
    "\n",
    "# Classify the images\n",
    "y_pred = np.array([])\n",
    "for img_path in paths:\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    # decode the results into a list of tuples (class, description, probability)\n",
    "    # (one such list for each sample in the batch)\n",
    "    label_pred = decode_predictions(preds, top=1)[0][0][1]\n",
    "    y_pred = np.append(y_pred, label_pred)\n",
    "\n",
    "print(\"y_true shape\", y_true.shape)\n",
    "print(\"y_pred shape\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "cm = pd.DataFrame(0, index=np.unique(y), columns=np.unique(y_pred))\n",
    "\n",
    "for true, pred in zip(y_true, y_pred):\n",
    "    cm.at[true, pred] += 1\n",
    "\n",
    "df_sum = pd.DataFrame(data=cm.sum(numeric_only=True)).T\n",
    "df_sum = df_sum.rename(index={0: 'SUM'})\n",
    "cm = cm.append(df_sum)\n",
    "\n",
    "cm = cm.sort_values(by=['SUM'], axis=1, ascending=False)\n",
    "\n",
    "cm = cm.drop('SUM', axis=0)\n",
    "\n",
    "cm.iloc[:, 0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "The assignment must be submitted as a [Jupyter notebook](http://jupyter.org/) through the following [Dropbox file request](https://www.dropbox.com/request/KqkTCJ1gENtNkNRaWomX), before midnight of the deadline date. The file must be named as ml-assign4-unalusername1-unalusername2.ipynb, where unalusername is the user name assigned by the university (include the usernames of all the members of the group). Check that the notebook executes properly and that the corresponding outputs render appropriately.  If there are several files, please put them in a zip file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
