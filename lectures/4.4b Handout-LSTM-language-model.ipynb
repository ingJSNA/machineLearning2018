{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM language model handout\n",
    "## [COSC 7336 Advanced Natural Language Processing](https://fagonzalezo.github.io/dl-tau-2017-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this handout we will build a character-based language model using a Long Short Term Memory (LSTM) recurrent neural network. The code is based on this Keras example https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py.\n",
    "\n",
    "We will use a text from Nietzsche that is availabe here: https://s3.amazonaws.com/text-datasets/nietzsche.txt. The text is converted to lowercase and we build a two dictionaries to map characters to indices and back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chars: 600893\n",
      "Vocabulary size: 57\n"
     ]
    }
   ],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"Total number of chars:\", len(text))\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of the test in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts object purely and simply as \"the thing in itself,\" without any\n",
      "falsification taking place either on the part of the subject or the\n",
      "object. i would repeat it, however, a hundred times, that \"immediate\n",
      "certainty,\" as well as \"absolute knowledge\" and the \"thing in itself,\"\n",
      "involve a contradictio in adjecto; we really ought to free ourselves\n",
      "from the misleading significance of words! the people on their part may\n",
      "think that cognition is knowing all about things, but the philosopher\n",
      "must say to him\n"
     ]
    }
   ],
   "source": [
    "print(text[31000:31500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Many to one LSTM model \n",
    "\n",
    "We will build a model with this structure:\n",
    "\n",
    "![many-to-one.jpg](img/many-to-one.jpg)\n",
    "\n",
    "The model receives sequences of size 40 and predicts the character that will follow this sequence, the 41-th character. The LSTM layer will have 128 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n",
      "Layer (type)                   Output Shape                Param #    \n",
      "======================================================================\n",
      "lstm_1 (LSTM)                  (None, 128)                 95232      \n",
      "______________________________________________________________________\n",
      "dense_1 (Dense)                (None, 57)                  7353       \n",
      "______________________________________________________________________\n",
      "activation_1 (Activation)      (None, 57)                  0          \n",
      "======================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, vocab_size), return_sequences=False, name=\"lstm_1\"))\n",
    "model.add(Dense(vocab_size, name=\"dense_1\"))\n",
    "model.add(Activation('softmax', name=\"activation_1\"))\n",
    "model.summary(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train this model we need to build the sequences from the input text. Each sequence will have a length of `maxlen = 40` and the label will correspond to the character that follows that sequence, i.e. the 41-th character. The sequences overlap, this is controlled with the `step` variable. Characters are represented using a one-hot representation of `len(chars)` length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 200285\n",
      "Vectorization...\n",
      "Shape X (200285, 40, 57)\n",
      "Shape y (200285, 57)\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('Shape X', X.shape)\n",
    "print('Shape y', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a categorical crossentropy loss since the output of the model is a softmax layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model will require at least 20 epochs to get good results, so a GPU is a most. For illustration we will train it for just an epoch with a reduced set of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3.7724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2fad666358>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[:1000,:,:], y[:1000,:], batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Many-to-many LSTM\n",
    "\n",
    "Despite the model we trained is a many-to-one model, we can convert it to a many-to-many model, since an LSTM always produces an output for every time step:\n",
    "\n",
    "![many-to-many.jpg](img/many-to-many.jpg)\n",
    "\n",
    "In order to do this we will change the architecture of the model. In particular, we will change the dense output layer so that it produces an output for every time step. This is accomplished by using the `TimeDistributed` class. This doesn't increase the number of parameters since the weights are shared for all the time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n",
      "Layer (type)                   Output Shape                Param #    \n",
      "======================================================================\n",
      "lstm_1 (LSTM)                  (None, 40, 128)             95232      \n",
      "______________________________________________________________________\n",
      "dense_1 (TimeDistributed)      (None, 40, 57)              7353       \n",
      "______________________________________________________________________\n",
      "activation_1 (Activation)      (None, 40, 57)              0          \n",
      "======================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, vocab_size), return_sequences=True, name=\"lstm_1\"))\n",
    "model.add(TimeDistributed(Dense(vocab_size), name=\"dense_1\"))#Check names to see how to load weights\n",
    "model.add(Activation('softmax', name=\"activation_1\"))\n",
    "model.summary(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load a pretrained model. This model corresponds to the many-to-one model define before, but since the parameters of both models are the same we can use the weights for the many-to-many model. You can obtain the model here: https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/lstm-pretrained-weights.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file = 'lstm-pretrained-weights.hdf5'\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.load_weights(h5file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating the probability of a text\n",
    "\n",
    "The model that we trained calculates the probability of a character given the previous characters: $P(c_i | c_{1},\\dots, c_{i-1})$. We can use this conditional probability to calculate the join probability of a given sequence as follows:\n",
    "$$P(c_1, \\dots, c_n) = P(c_1)\\prod_{i=2}^{n}\\ P(c_i | c_{1},\\dots, c_{i-1})$$\n",
    "\n",
    "This probability is the likelihood of the particular sequence $c_1, \\dots, c_n$ being generated by the language model.\n",
    "\n",
    "In the many-to-many model the conditional probabilities are given by the outputs at each time step. Remember that the output of the model is a softmax layer with as many neurons are characters in the vocabulary. The following function calculates the log likelihood of a text according to the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text):\n",
    "    probs = model.predict(parse_text(text, vocab_size, padding=True)).squeeze()\n",
    "    return sum([np.log(probs[i, char_indices[c]]) \n",
    "                 for i,c in enumerate(text[1:]) ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it for particular texts, we need to represent them in a way that is understood by the neural network model, this is done by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, vocab_size, padding=False):\n",
    "    if padding:\n",
    "        X = np.zeros((1, maxlen, vocab_size), dtype=np.bool)\n",
    "    else:\n",
    "        X = np.zeros((1, len(text), vocab_size), dtype=np.bool)\n",
    "    for t, char in enumerate(text):\n",
    "        X[0, t, char_indices[char]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute value of the likelihood does not say much by itself. It is more useful if we use it to compare different texts, i.e. if we interpret it in relative terms.\n",
    "\n",
    "For instance the likelihood of the text *\"the thing in itself\"* is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.652141031925566\n"
     ]
    }
   ],
   "source": [
    "print (log_likelihood(model, \"the thing in itself\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the likelihood of the same text without spaces, *\"thethinginitself\"*, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-59.05251658335328\n"
     ]
    }
   ],
   "source": [
    "print (log_likelihood(model, \"thethinginitself\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that $$P(\\text{\"the thing in itself\"} | model) > P(\\text{\"thethinginitself\"} | model)$$\n",
    "\n",
    "The following are other variations of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-30.801376580726355\n",
      "-31.79169424250722\n"
     ]
    }
   ],
   "source": [
    "print (log_likelihood(model, \"the thingy in itself\"))\n",
    "print (log_likelihood(model, \"itself thing the in\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggest that the model is able to capture different aspects of the text structure such as the morphology of the words or the syntaxis.\n",
    "\n",
    "### 3.1 Morphological structure\n",
    "\n",
    "The model can help us to identify the most likely (unlikely) combinations of a set of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.907237097620964 y wh\n",
      "-4.965351223945618 why \n",
      "-7.635346040129662  why\n",
      "-7.640632808208466 hy w\n",
      "-9.284396946430206 y hw\n",
      "--------------------------------------------------\n",
      "-18.857420921325684  ywh\n",
      "-19.525946140289307 wyh \n",
      "-21.43775177001953 h wy\n",
      "-21.46756076812744 h yw\n",
      "-25.410496711730957  yhw\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "char_list = list(u'ywh ')\n",
    "perms = [''.join(perm) for perm in permutations(char_list)]\n",
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "print('-'*50)\n",
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Syntactical structure\n",
    "\n",
    "Instead of characters we can use words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "bow =  ['philosopher', 'kant', 'is', 'a']\n",
    "perms = [' '+' '.join(perm)+' ' for perm in permutations(bow)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most likely permutations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.92482476151781  is a philosopher kant \n",
      "-32.32370726700174  kant is a philosopher \n",
      "-34.8795671366388  a kant is philosopher \n",
      "-35.60372036101762  kant is philosopher a \n",
      "-39.971708870027214  a philosopher kant is \n",
      "-43.284406012215186  is philosopher kant a \n",
      "-43.675346034462564  is kant a philosopher \n",
      "-43.7594948079859  a kant philosopher is \n",
      "-43.83798664994538  is a kant philosopher \n",
      "-44.60559724805353  a is kant philosopher \n"
     ]
    }
   ],
   "source": [
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse = True)[:10]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most unlikely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-50.76168235053774  kant a is philosopher \n",
      "-51.16493713911041  kant philosopher is a \n",
      "-53.60183866764419  is kant philosopher a \n",
      "-58.62756750360131  philosopher kant is a \n",
      "-65.44321207143366  philosopher is a kant \n",
      "-66.78526555374265  philosopher a kant is \n",
      "-69.44112966264947  kant philosopher a is \n",
      "-71.22503350302577  philosopher is kant a \n",
      "-71.66534561291337  philosopher kant a is \n",
      "-76.04576717503369  philosopher a is kant \n"
     ]
    }
   ],
   "source": [
    "for p, t in sorted([(log_likelihood(model, text), text) for text in perms], reverse = True)[-10:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating text\n",
    "\n",
    "One of the most interesting applications of language models is random text generation. \n",
    "\n",
    "Since the model tells us the probability of the next character given the previous ones. We can start from a given text and use the model conditional probability to get a good candidate for the next character, we add that character to our sequence and we keep generating characters in this way.\n",
    "\n",
    "To generate a character using the conditional probability calculated by the model, $P(c_t | c_{1}, c_{2},\\dots, c_{t-1})$, we will use two main mechanisms: deterministic and  stochastic. For the deterministic we generate the character with the maximum probability. For the stochastic we sample from the conditional probability distribution given by the model. We combine the two strategies using a process analogous to this one:\n",
    "  ```python\n",
    "  for i in [1..n]:\n",
    "      P = predict_next() \n",
    "      bin_var = sample_binomial(temperature)\n",
    "      if bin_var:\n",
    "          c_i = sample_multinomial(P) \n",
    "      else:\n",
    "          c_i = P.argmax() \n",
    "  ```\n",
    "The `temperature` parameter determines the balance between stochastic and deterministic. A higher value makes the process more stochastic, a lower value makes it more deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample an index from a probability array:\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function feeds the model with an initial text, successively execute the model to get a probability distribution and uses the `sample` function to generate the next character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(diversity, model, sentence, n_chars, padding=True):\n",
    "    print()\n",
    "    print(sentence, end='')\n",
    "    for i in range(n_chars):\n",
    "        x = np.zeros((1, maxlen, vocab_size))\n",
    "        if padding and len(sentence) < 40:\n",
    "            space_array = [\" \"]*(40-len(sentence))\n",
    "            for t, char in enumerate(space_array):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "        for t, char in enumerate(sentence, 40-len(sentence)):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds[-1], diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask the model for the *'the meaning of life'* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the meaning of life is the morality of the motion, the sensible\n",
      "say have been but to be the sympay has for the actions he restlays his power what the highest thingst man who know hitho[the mistimutud and fave and the who, for great higher had not so detraide\" he conditions of the histlees who of looking to be stances there is possibly who have being sin5x the same than constination, that is \"symparwi\"s i sedence of the "
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(0.5, model, 'the meaning of life is ', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
